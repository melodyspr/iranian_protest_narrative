{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdfd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_en = pd.read_csv(\"data/en_duplicates_chunks_averagerep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55732b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "# Load your DataFrame (assuming df is already loaded)\n",
    "# Step 1: Filter group_duplicate == 136\n",
    "df_1 = df_en[df_en[\"group_rank\"] == 1]\n",
    "\n",
    "# Step 2: Random sample of 100 tweets\n",
    "sample_df = df_1.sample(n=100, random_state=42)\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r\"@\\w+\", '', text) \n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'#مهسا_امینی\\b', '', text)\n",
    "    \n",
    "    # Remove non-alphabetic characters (keeping spaces)\n",
    "    # Adjust this based on whether you want to keep numbers or other characters\n",
    "    #text = re.sub(r\"[^a-zA-Zà-ÿء-يآ\\s]\", '', text)\n",
    "    \n",
    "    # Remove extra whitespace (multiple spaces, tabs, newlines)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Apply the function\n",
    "sample_df[\"tweet_clean_graph\"] = sample_df[\"tweet\"].apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd1a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "tweets = sample_df[\"tweet_clean_graph\"].tolist()\n",
    "# Step 2: Build directed co-occurrence graph\n",
    "co_occur = defaultdict(int)\n",
    "for tweet in tweets:\n",
    "    words = tweet\n",
    "    for i in range(len(words) - 1):\n",
    "        word1, word2 = words[i], words[i + 1]\n",
    "        co_occur[(word1, word2)] += 1\n",
    "\n",
    "# Step 3: Create directed graph G\n",
    "G = nx.DiGraph()\n",
    "for (word1, word2), weight in co_occur.items():\n",
    "    G.add_edge(word1, word2, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_words = [\"22\", \"twenty\", \"two\", \"twenty-two\", \"twentytwo\", \"years\", \"old\", \"yearsold\", \"yearold\", \"year\", \"yo\", \"iranian\", \"girl\", \"is\", \"brutally\", \"killed\", \"by\", \"sharia\", \"police\", \"in\", \"iran\", \"iran.\"]\n",
    "\n",
    "def classify_word(word):\n",
    "    if re.match(r\"@[\\w_]+\", word):\n",
    "        return \"mention\"\n",
    "    elif re.match(r\"#\\w+\", word):\n",
    "        return \"hashtag\"\n",
    "    elif re.search(r\"[اآبپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی]\", word):  # contains Persian chars\n",
    "        return \"persian\"\n",
    "    elif word in core_words:\n",
    "        return \"core\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# Define color map\n",
    "color_map = {\n",
    "    \"core\": \"#E74C3C\",       # red\n",
    "    \"mention\": \"#3498DB\",    # blue\n",
    "    \"hashtag\": \"#F1C40F\",    # yellow\n",
    "    \"persian\": \"#9B59B6\",    # purple\n",
    "    \"other\": \"#BDC3C7\"       # light gray\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d75fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Assign node attributes for Gephi\n",
    "for node in G.nodes():\n",
    "    category = classify_word(node)\n",
    "    G.nodes[node]['category'] = category  # useful for Gephi coloring\n",
    "    G.nodes[node]['label'] = node         # ensures node labels are visible in Gephi\n",
    "    G.nodes[node]['degree'] = G.degree(node)  # size nodes by degree if desired\n",
    "\n",
    "# Step 5: Export to GEXF format for Gephi\n",
    "nx.write_gexf(G, \"figures/cooccurrence_graph_gephi_100.gexf\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
