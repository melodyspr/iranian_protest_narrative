{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3c74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Evaluating on 10,000 Persian tweets (8,826 eligible for matching; 1,174 below 5 words).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 5 single-member groups as non-duplicates\n",
      "Threshold 75: 131 groups | duplicates = 387 (3.87%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 5 single-member groups as non-duplicates\n",
      "Threshold 76: 132 groups | duplicates = 386 (3.86%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 5 single-member groups as non-duplicates\n",
      "Threshold 77: 132 groups | duplicates = 385 (3.85%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 5 single-member groups as non-duplicates\n",
      "Threshold 78: 131 groups | duplicates = 383 (3.83%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 4 single-member groups as non-duplicates\n",
      "Threshold 79: 128 groups | duplicates = 377 (3.77%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 3 single-member groups as non-duplicates\n",
      "Threshold 80: 128 groups | duplicates = 377 (3.77%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 3 single-member groups as non-duplicates\n",
      "Threshold 81: 128 groups | duplicates = 373 (3.73%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 2 single-member groups as non-duplicates\n",
      "Threshold 82: 125 groups | duplicates = 363 (3.63%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 1 single-member groups as non-duplicates\n",
      "Threshold 83: 125 groups | duplicates = 362 (3.62%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 0 single-member groups as non-duplicates\n",
      "Threshold 84: 123 groups | duplicates = 356 (3.56%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 2841 short tweets (<10 words) as non-duplicates\n",
      "Marked 0 single-member groups as non-duplicates\n",
      "Threshold 85: 122 groups | duplicates = 350 (3.50%)\n",
      "\n",
      "✅ Results saved to: data/duplicate_threshold_summary_10k_20251029_111220.csv\n",
      "\n",
      "Summary:\n",
      "  Threshold  Duplicate_Groups  Duplicate_Tweets  Total_Tweets  %Duplicates  %Non_Duplicates\n",
      "        75               131               387         10000         3.87            96.13\n",
      "        76               132               386         10000         3.86            96.14\n",
      "        77               132               385         10000         3.85            96.15\n",
      "        78               131               383         10000         3.83            96.17\n",
      "        79               128               377         10000         3.77            96.23\n",
      "        80               128               377         10000         3.77            96.23\n",
      "        81               128               373         10000         3.73            96.27\n",
      "        82               125               363         10000         3.63            96.37\n",
      "        83               125               362         10000         3.62            96.38\n",
      "        84               123               356         10000         3.56            96.44\n",
      "        85               122               350         10000         3.50            96.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# Basic cleaning (no normalization)\n",
    "# -----------------------------\n",
    "URL_RE = re.compile(r'http\\S+')\n",
    "MENTION_RE = re.compile(r'@\\w+')\n",
    "HASHTAG_RE = re.compile(r'#\\w+')\n",
    "\n",
    "def clean_tweet(text: str) -> str:\n",
    "    text = str(text).lower()\n",
    "    text = URL_RE.sub('', text)\n",
    "    text = MENTION_RE.sub('', text)\n",
    "    text = HASHTAG_RE.sub('', text)\n",
    "    return ' '.join(text.split()).strip()\n",
    "\n",
    "# -----------------------------\n",
    "# Robustness helpers\n",
    "# -----------------------------\n",
    "def clean_short_tweets(df: pd.DataFrame, min_words: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Mark tweets with <min_words as non-duplicates.\"\"\"\n",
    "    df = df.copy()\n",
    "    short_mask = df['word_count'] < min_words\n",
    "    df.loc[short_mask, 'group'] = -1\n",
    "    df.loc[short_mask, 'group_rank'] = -1\n",
    "    df.loc[short_mask, 'is_duplicate'] = False\n",
    "    print(f\"Marked {short_mask.sum()} short tweets (<{min_words} words) as non-duplicates\")\n",
    "    return df\n",
    "\n",
    "def mark_single_member_groups_as_non_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mark single-member duplicate groups as non-duplicates.\"\"\"\n",
    "    df = df.copy()\n",
    "    group_sizes = df[df['group'] != -1]['group'].value_counts()\n",
    "    single_member_groups = group_sizes[group_sizes == 1].index\n",
    "    if len(single_member_groups):\n",
    "        mask = df['group'].isin(single_member_groups)\n",
    "        df.loc[mask, 'group'] = -1\n",
    "        df.loc[mask, 'group_rank'] = -1\n",
    "        df.loc[mask, 'is_duplicate'] = False\n",
    "    print(f\"Marked {len(single_member_groups)} single-member groups as non-duplicates\")\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Duplicate detection (single pass, no chunks)\n",
    "# -----------------------------\n",
    "def group_duplicates_single(df_proc: pd.DataFrame, threshold: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns a pandas Series 'group' aligned with df_proc.index.\n",
    "    Duplicate tweets (multi-member groups) get a non-negative group id; singletons get -1.\n",
    "    \"\"\"\n",
    "    tweet_list = df_proc['tweet_clean'].tolist()\n",
    "    indices = df_proc.index.tolist()\n",
    "    groups = {}  # idx -> group_id (use smallest idx in group)\n",
    "\n",
    "    for i in tqdm(range(len(tweet_list)), desc=f\"Scanning @{threshold}\", leave=False):\n",
    "        idx_i = indices[i]\n",
    "        if idx_i in groups:\n",
    "            continue\n",
    "\n",
    "        ti = tweet_list[i]\n",
    "        li = len(ti)\n",
    "        if li == 0:\n",
    "            continue\n",
    "\n",
    "        members = [idx_i]\n",
    "\n",
    "        for j in range(i + 1, len(tweet_list)):\n",
    "            idx_j = indices[j]\n",
    "            if idx_j in groups:\n",
    "                continue\n",
    "\n",
    "            tj = tweet_list[j]\n",
    "            # length prefilter (±30%)\n",
    "            if abs(len(tj) - li) <= 0.3 * li:\n",
    "                score = fuzz.ratio(ti, tj)\n",
    "                if score >= threshold:\n",
    "                    members.append(idx_j)\n",
    "\n",
    "        if len(members) > 1:\n",
    "            gid = min(members)\n",
    "            for m in members:\n",
    "                groups[m] = gid\n",
    "\n",
    "    # Build group Series\n",
    "    group_ser = pd.Series(-1, index=df_proc.index, dtype=int)\n",
    "    if groups:\n",
    "        for idx, gid in groups.items():\n",
    "            group_ser.at[idx] = gid\n",
    "\n",
    "        # remove singleton groups (defensive)\n",
    "        counts = group_ser[group_ser != -1].value_counts()\n",
    "        singletons = counts[counts == 1].index\n",
    "        group_ser.loc[group_ser.isin(singletons)] = -1\n",
    "\n",
    "        # relabel consecutive\n",
    "        valid = group_ser[group_ser != -1].unique()\n",
    "        mapping = {old: new for new, old in enumerate(sorted(valid))}\n",
    "        group_ser = group_ser.map(lambda x: mapping.get(x, -1))\n",
    "\n",
    "    return group_ser\n",
    "\n",
    "# -----------------------------\n",
    "# Threshold sweep with saving + robustness\n",
    "# -----------------------------\n",
    "def evaluate_thresholds(df_fa: pd.DataFrame,\n",
    "                        thresholds=range(75, 86),\n",
    "                        sample_n=10000,\n",
    "                        seed=42,\n",
    "                        min_words_for_matching=5,\n",
    "                        min_words_robust=10,\n",
    "                        save_path=\"data/duplicate_threshold_summary.csv\"):\n",
    "    \"\"\"\n",
    "    Samples tweets, sweeps thresholds, and saves summary (threshold, % duplicates, n groups, etc.)\n",
    "    Applies robustness checks before counting.\n",
    "    \"\"\"\n",
    "    # Sample up to 10k\n",
    "    df = df_fa.sample(min(sample_n, len(df_fa)), random_state=seed).copy()\n",
    "    df['tweet_clean'] = df['tweet'].map(clean_tweet)\n",
    "    df['word_count'] = df['tweet_clean'].str.split().str.len()\n",
    "\n",
    "    eligible_mask = df['word_count'] >= min_words_for_matching\n",
    "    df_proc = df[eligible_mask].copy()\n",
    "    df_short = df[~eligible_mask].copy()\n",
    "    N_total = len(df)\n",
    "\n",
    "    print(f\"Evaluating on {N_total:,} Persian tweets \"\n",
    "          f\"({len(df_proc):,} eligible for matching; {len(df_short):,} below {min_words_for_matching} words).\")\n",
    "\n",
    "    results = []\n",
    "    for thr in thresholds:\n",
    "        if len(df_proc) == 0:\n",
    "            pct_dup = 0.0\n",
    "            n_groups = 0\n",
    "            dup_count = 0\n",
    "        else:\n",
    "            # 1. Group duplicates on eligible tweets\n",
    "            groups = group_duplicates_single(df_proc, threshold=thr)\n",
    "            work = df.copy()\n",
    "            work['group'] = -1\n",
    "            work.loc[groups.index, 'group'] = groups.values\n",
    "            work['group_rank'] = -1\n",
    "            work['is_duplicate'] = work['group'] != -1\n",
    "\n",
    "            # 2. Apply robustness checks\n",
    "            work = clean_short_tweets(work, min_words=min_words_robust)\n",
    "            work = mark_single_member_groups_as_non_duplicates(work)\n",
    "\n",
    "            # 3. Compute counts\n",
    "            dup_mask = work['group'] != -1\n",
    "            dup_count = dup_mask.sum()\n",
    "            n_groups = work.loc[dup_mask, 'group'].nunique()\n",
    "            pct_dup = 100.0 * dup_count / N_total\n",
    "\n",
    "        pct_non = 100.0 - pct_dup\n",
    "        results.append((thr, n_groups, dup_count, N_total, pct_dup, pct_non))\n",
    "        print(f\"Threshold {thr}: {n_groups} groups | duplicates = {dup_count:,} ({pct_dup:.2f}%)\")\n",
    "\n",
    "    out = pd.DataFrame(results, columns=[\n",
    "        \"Threshold\", \"Duplicate_Groups\", \"Duplicate_Tweets\", \"Total_Tweets\", \"%Duplicates\", \"%Non_Duplicates\"\n",
    "    ])\n",
    "\n",
    "    # Save results with timestamp\n",
    "    #timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    #save_file = save_path.replace(\".csv\", f\"_{timestamp}.csv\")\n",
    "    out.to_csv(save_path, index=False)\n",
    "    print(f\"\\n✅ Results saved to: {save_path}\")\n",
    "    print(\"\\nSummary:\\n\", out.to_string(index=False))\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading dataset...\")\n",
    "    df_all = pd.read_pickle(\"data/persian_english_tweets_onehashtag_twomonths_processed.pkl\")\n",
    "    df_fa = df_all[df_all[\"lang\"] == \"fa\"].dropna(subset=[\"tweet\"]).reset_index(drop=True)\n",
    "\n",
    "    _ = evaluate_thresholds(\n",
    "        df_fa,\n",
    "        thresholds=range(75, 86),     # test thresholds 75–85 inclusive\n",
    "        sample_n=10000,\n",
    "        seed=42,\n",
    "        min_words_for_matching=5,     # eligible for duplicate comparison\n",
    "        min_words_robust=10,          # force <10 words to non-duplicate\n",
    "        save_path=\"data/duplicate_threshold_summary_10k.csv\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6f37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 100,000 Persian tweets (88,390 eligible for matching; 11,610 below 5 words).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 28718 short tweets (<10 words) as non-duplicates\n",
      "Marked 34 single-member groups as non-duplicates\n",
      "Threshold 86: 2669 groups | duplicates = 10,028 (10.03%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 28718 short tweets (<10 words) as non-duplicates\n",
      "Marked 23 single-member groups as non-duplicates\n",
      "Threshold 87: 2675 groups | duplicates = 9,959 (9.96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 28718 short tweets (<10 words) as non-duplicates\n",
      "Marked 20 single-member groups as non-duplicates\n",
      "Threshold 88: 2668 groups | duplicates = 9,853 (9.85%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 28718 short tweets (<10 words) as non-duplicates\n",
      "Marked 18 single-member groups as non-duplicates\n",
      "Threshold 89: 2646 groups | duplicates = 9,705 (9.71%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 28718 short tweets (<10 words) as non-duplicates\n",
      "Marked 18 single-member groups as non-duplicates\n",
      "Threshold 90: 2628 groups | duplicates = 9,568 (9.57%)\n",
      "\n",
      "✅ Results saved to: data/duplicate_threshold_summary_100k_2_20251030_131524.csv\n",
      "\n",
      "Summary:\n",
      "  Threshold  Duplicate_Groups  Duplicate_Tweets  Total_Tweets  %Duplicates  %Non_Duplicates\n",
      "        86              2669             10028        100000       10.028           89.972\n",
      "        87              2675              9959        100000        9.959           90.041\n",
      "        88              2668              9853        100000        9.853           90.147\n",
      "        89              2646              9705        100000        9.705           90.295\n",
      "        90              2628              9568        100000        9.568           90.432\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# Basic cleaning (no normalization)\n",
    "# -----------------------------\n",
    "URL_RE = re.compile(r'http\\S+')\n",
    "MENTION_RE = re.compile(r'@\\w+')\n",
    "HASHTAG_RE = re.compile(r'#\\w+')\n",
    "\n",
    "def clean_tweet(text: str) -> str:\n",
    "    text = str(text).lower()\n",
    "    text = URL_RE.sub('', text)\n",
    "    text = MENTION_RE.sub('', text)\n",
    "    text = HASHTAG_RE.sub('', text)\n",
    "    return ' '.join(text.split()).strip()\n",
    "\n",
    "# -----------------------------\n",
    "# Robustness helpers\n",
    "# -----------------------------\n",
    "def clean_short_tweets(df: pd.DataFrame, min_words: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Mark tweets with <min_words as non-duplicates.\"\"\"\n",
    "    df = df.copy()\n",
    "    short_mask = df['word_count'] < min_words\n",
    "    df.loc[short_mask, 'group'] = -1\n",
    "    df.loc[short_mask, 'group_rank'] = -1\n",
    "    df.loc[short_mask, 'is_duplicate'] = False\n",
    "    print(f\"Marked {short_mask.sum()} short tweets (<{min_words} words) as non-duplicates\")\n",
    "    return df\n",
    "\n",
    "def mark_single_member_groups_as_non_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Mark single-member duplicate groups as non-duplicates.\"\"\"\n",
    "    df = df.copy()\n",
    "    group_sizes = df[df['group'] != -1]['group'].value_counts()\n",
    "    single_member_groups = group_sizes[group_sizes == 1].index\n",
    "    if len(single_member_groups):\n",
    "        mask = df['group'].isin(single_member_groups)\n",
    "        df.loc[mask, 'group'] = -1\n",
    "        df.loc[mask, 'group_rank'] = -1\n",
    "        df.loc[mask, 'is_duplicate'] = False\n",
    "    print(f\"Marked {len(single_member_groups)} single-member groups as non-duplicates\")\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Duplicate detection (single pass, no chunks)\n",
    "# -----------------------------\n",
    "def group_duplicates_single(df_proc: pd.DataFrame, threshold: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returns a pandas Series 'group' aligned with df_proc.index.\n",
    "    Duplicate tweets (multi-member groups) get a non-negative group id; singletons get -1.\n",
    "    \"\"\"\n",
    "    tweet_list = df_proc['tweet_clean'].tolist()\n",
    "    indices = df_proc.index.tolist()\n",
    "    groups = {}  # idx -> group_id (use smallest idx in group)\n",
    "\n",
    "    for i in tqdm(range(len(tweet_list)), desc=f\"Scanning @{threshold}\", leave=False):\n",
    "        idx_i = indices[i]\n",
    "        if idx_i in groups:\n",
    "            continue\n",
    "\n",
    "        ti = tweet_list[i]\n",
    "        li = len(ti)\n",
    "        if li == 0:\n",
    "            continue\n",
    "\n",
    "        members = [idx_i]\n",
    "\n",
    "        for j in range(i + 1, len(tweet_list)):\n",
    "            idx_j = indices[j]\n",
    "            if idx_j in groups:\n",
    "                continue\n",
    "\n",
    "            tj = tweet_list[j]\n",
    "            # length prefilter (±30%)\n",
    "            if abs(len(tj) - li) <= 0.3 * li:\n",
    "                score = fuzz.ratio(ti, tj)\n",
    "                if score >= threshold:\n",
    "                    members.append(idx_j)\n",
    "\n",
    "        if len(members) > 1:\n",
    "            gid = min(members)\n",
    "            for m in members:\n",
    "                groups[m] = gid\n",
    "\n",
    "    # Build group Series\n",
    "    group_ser = pd.Series(-1, index=df_proc.index, dtype=int)\n",
    "    if groups:\n",
    "        for idx, gid in groups.items():\n",
    "            group_ser.at[idx] = gid\n",
    "\n",
    "        # remove singleton groups (defensive)\n",
    "        counts = group_ser[group_ser != -1].value_counts()\n",
    "        singletons = counts[counts == 1].index\n",
    "        group_ser.loc[group_ser.isin(singletons)] = -1\n",
    "\n",
    "        # relabel consecutive\n",
    "        valid = group_ser[group_ser != -1].unique()\n",
    "        mapping = {old: new for new, old in enumerate(sorted(valid))}\n",
    "        group_ser = group_ser.map(lambda x: mapping.get(x, -1))\n",
    "\n",
    "    return group_ser\n",
    "\n",
    "# -----------------------------\n",
    "# Threshold sweep with saving + robustness\n",
    "# -----------------------------\n",
    "def evaluate_thresholds(df_fa: pd.DataFrame,\n",
    "                        thresholds=range(75, 86),\n",
    "                        sample_n=10000,\n",
    "                        seed=42,\n",
    "                        min_words_for_matching=5,\n",
    "                        min_words_robust=10,\n",
    "                        save_path=\"data/duplicate_threshold_summary.csv\"):\n",
    "    \"\"\"\n",
    "    Samples tweets, sweeps thresholds, and saves summary (threshold, % duplicates, n groups, etc.)\n",
    "    Applies robustness checks before counting.\n",
    "    \"\"\"\n",
    "    # Sample up to 10k\n",
    "    df = df_fa.sample(min(sample_n, len(df_fa)), random_state=seed).copy()\n",
    "    df['tweet_clean'] = df['tweet'].map(clean_tweet)\n",
    "    df['word_count'] = df['tweet_clean'].str.split().str.len()\n",
    "\n",
    "    eligible_mask = df['word_count'] >= min_words_for_matching\n",
    "    df_proc = df[eligible_mask].copy()\n",
    "    df_short = df[~eligible_mask].copy()\n",
    "    N_total = len(df)\n",
    "\n",
    "    print(f\"Evaluating on {N_total:,} Persian tweets \"\n",
    "          f\"({len(df_proc):,} eligible for matching; {len(df_short):,} below {min_words_for_matching} words).\")\n",
    "\n",
    "    results = []\n",
    "    for thr in thresholds:\n",
    "        if len(df_proc) == 0:\n",
    "            pct_dup = 0.0\n",
    "            n_groups = 0\n",
    "            dup_count = 0\n",
    "        else:\n",
    "            # 1. Group duplicates on eligible tweets\n",
    "            groups = group_duplicates_single(df_proc, threshold=thr)\n",
    "            work = df.copy()\n",
    "            work['group'] = -1\n",
    "            work.loc[groups.index, 'group'] = groups.values\n",
    "            work['group_rank'] = -1\n",
    "            work['is_duplicate'] = work['group'] != -1\n",
    "\n",
    "            # 2. Apply robustness checks\n",
    "            work = clean_short_tweets(work, min_words=min_words_robust)\n",
    "            work = mark_single_member_groups_as_non_duplicates(work)\n",
    "\n",
    "            # 3. Compute counts\n",
    "            dup_mask = work['group'] != -1\n",
    "            dup_count = dup_mask.sum()\n",
    "            n_groups = work.loc[dup_mask, 'group'].nunique()\n",
    "            pct_dup = 100.0 * dup_count / N_total\n",
    "\n",
    "        pct_non = 100.0 - pct_dup\n",
    "        results.append((thr, n_groups, dup_count, N_total, pct_dup, pct_non))\n",
    "        print(f\"Threshold {thr}: {n_groups} groups | duplicates = {dup_count:,} ({pct_dup:.2f}%)\")\n",
    "\n",
    "    out = pd.DataFrame(results, columns=[\n",
    "        \"Threshold\", \"Duplicate_Groups\", \"Duplicate_Tweets\", \"Total_Tweets\", \"%Duplicates\", \"%Non_Duplicates\"\n",
    "    ])\n",
    "\n",
    "    # Save results with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_file = save_path.replace(\".csv\", f\"_{timestamp}.csv\")\n",
    "    out.to_csv(save_file, index=False)\n",
    "    print(f\"\\n✅ Results saved to: {save_file}\")\n",
    "    print(\"\\nSummary:\\n\", out.to_string(index=False))\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # print(\"Loading dataset...\")\n",
    "    # df_all = pd.read_pickle(\"data/persian_english_tweets_onehashtag_twomonths_processed.pkl\")\n",
    "    # df_fa = df_all[df_all[\"lang\"] == \"fa\"].dropna(subset=[\"tweet\"]).reset_index(drop=True)\n",
    "\n",
    "    _ = evaluate_thresholds(\n",
    "        df_fa,\n",
    "        thresholds=range(75, 91),     # test thresholds 75–85 inclusive\n",
    "        sample_n=100000,\n",
    "        seed=42,\n",
    "        min_words_for_matching=5,     # eligible for duplicate comparison\n",
    "        min_words_robust=10,          # force <10 words to non-duplicate\n",
    "        save_path=\"data/duplicate_threshold_summary_100k.csv\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persiantwitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
